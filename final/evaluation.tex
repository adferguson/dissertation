% !TEX root = ferguson-dissertation.tex
\chapter{Evaluation of Participatory Networking}
\label{sec:Evaluation}

%\remark{
%{\em Expected Outcome:} Reader believes that participatory networking provides 
%benefits to real applications, and that it is feasible to implement such a 
%system in today's networks.
%}

We evaluate our \sys prototype with the Mininet platform for
emulating SDNs~\cite{Mininet}, and with real networks. Our primary
testbed includes two Pronto 3290 switches running the Indigo
firmware,\footnote{\scriptsize{\url{http://indigo.openflowhub.org/}}}
and several software OpenFlow switches (both Open vSwitch and the
reference user-mode switch) running
on Linux Intel-compatible
hardware, and on the TP-Link WR-1043ND wireless router.  Wired
connections are 1 Gbps and wireless runs over 802.11n. Clients on
the network include dedicated Linux servers, and fluctuating numbers
of personal laptops and phones. In addition to the participatory
networking API, the network also provides standard services such as
DHCP, DNS, and NAT.

Members of our group have been using the testbed since February 2012
to manage our traffic, and during this time, it has
been our primary source of network connectivity. The testbed is
compatible with unmodified consumer electronic devices, which can
easily interact with a \sys controller running at a well-known
location.\footnote{The \sys controller could also be specified using
a DHCP vendor-specific or site-specific option.}

In the following chapters, we examine two aspects of our prototype.
First, we consider four case studies of real applications
that use the \sys API to improve end-user experience
(\xref{sec:ApplicationUsage}).  Second, we evaluate the practicality
of implementing the \sys API in current OpenFlow-enabled networks,
considering questions such as the latency of processing requests,
and the number of rules created by networked applications
(\xref{sec:Microbenchmarks}).

\section{Application Usage}
\label{sec:ApplicationUsage}

We ported four real applications to use the \sys API: Ekiga,
SSHGuard, ZooKeeper, and Hadoop. We now describe how intentions of
an application developer or user can be translated to our API, and
the effects of using \sys on the network and the application. Our
\sys-enabled versions of these applications are all publicly
available on Github.\footnote{\url{http://github.com/brownsys}.}

\subsection{Ekiga}

Ekiga is an open source video conferencing application. %\footnote{http://ekiga.org}
We modified Ekiga to ask the user for the anticipated duration of
video calls,
% when it is placed.
and use a \priv{Reserve} message to request guaranteed bandwidth
from the network between the caller's host and either the network
gateway or the recipient's host, for the appropriate time. If such a
reservation is not available, Ekiga retrieves the schedule of
available bandwidth from \sys and calculates the earliest time at
which a video call or, alternatively, an audio call, can be made
with guaranteed quality. It then presents these options to the
user, along with a third option for placing a ``best effort'' call
right away.
%Our modified version of Ekiga is available on Github.

Realizable reservations cause the \sys controller to create
guaranteed bandwidth queues along the path of the circuit, and
install forwarding rules for Ekiga's traffic.
Measurements of Skype use on a campus network with more than 7000
hosts show that making reservations with \sys for VoIP applications
is quite feasible. Skype calls peaked at 75 per hour, with 80\% of
calls lasting for fewer than 30 minutes~\cite{SkypeTraffic}.  This
frequency is well within current OpenFlow switches' capabilities, as
we measure in \xref{sec:Microbenchmarks}.\\

\subsection{SSHGuard}
\label{sec:sshguard}

SSHGuard is a popular tool to detect brute-force attacks via log monitoring
and install local firewall rules (\eg, via \verb/iptables/)
in response. We modified SSHGuard to use
\sys as a firewall backend to block nefarious traffic entering
the network. In particular, this means such traffic no longer traverses the
targeted host's access link.

For example, if Alice is running SSHGuard on her host and
it detects a Linux syslog entry such as:

\begin{small}
\begin{verbatim}
sshd[2197]: Invalid user Eve from 10.0.0.3
\end{verbatim}
\end{small}
%
SSHGuard will block Eve's traffic for the next five minutes using
\sys's \priv{Deny} request. The \sys controller then places an
OpenFlow rule to drop packets to Alice's host coming from Eve's at a
switch close to Eve's host.
%, following the procedure in \xref{sec:NIB}.

Although this is a basic example, it illustrates \sys's ability to
expose in-network functionality (namely, dropping packets) to
end-user applications.  Besides off-loading work from the end-host's
network stack, this approach also protects any innocent traffic
which might have suffered due to sharing a network link with a
denial-of-service (DoS) attack.

To demonstrate this benefit, we generated a UDP-based DoS attack
within our testbed network. We started an \verb/iperf/ TCP transfer between
two wireless clients, measured initially at 24 Mbps. We then
launched the attack from a Linux server two switch-hops away from
the wireless clients. During the attack, which was directed at one
of the clients, the performance of the \verb/iperf/ transfer dropped to 5
Mbps, rising to only 8 Mbps after the victim installed a local
firewall rule.  By using \sys to block the attack, the transfer's
full bandwidth returned.

\subsection{ZooKeeper}

ZooKeeper~\cite{Hunt:2010} is a coordination service for distributed systems 
used by Twitter, Netflix, and Yahoo!, among others, and is a key component of 
HBase. Like other coordination services such as Paxos~\cite{Lamport:1998}, 
ZooKeeper provides consistent, available, and shared state using a quorum of 
replicated servers (the \emph{ensemble}). For resiliency in the face of network 
failures, ZooKeeper servers may be distributed throughout a datacenter, and 
thus quorum messages may be negatively affected by heavy traffic on shared 
links. Because ZooKeeper's role is to provide coordination for other services, 
such negative effects are undesirable.

To protect ZooKeeper's messages from heavy traffic on shared links, we modified 
ZooKeeper to make bandwidth reservations using \sys. Upon startup, each member 
of the ensemble made a reservation for 10 Mbps of guaranteed minimum bandwidth for 
messages with other ZooKeeper servers. Additionally, we modified our ZooKeeper 
client to make a similar reservation with each server it connected to.

% We installed an ensemble of five ZooKeeper servers, running on \emph{host1-5}
% in {\color{red}Figure: \sys Network}, and developed a benchmarking client which
% we ran on \emph{\sys-nat}.
We installed ZooKeeper on an ensemble of five servers, and developed a 
benchmarking client which we ran on a sixth. The client connected a thread to 
each server and maximized the throughput of synchronous ZooKeeper operations in 
our ensemble.
To remove the effect of disk latency, the ZooKeeper servers used
RAM disks for storage.
At no time during these experiments were the CPUs of the client, 
switches, or servers fully loaded.
Like our modified applications, this benchmarking tool is also available on Github.
%Both our benchmark client and \sys-enabled
%version of ZooKeeper are available on Github.

\begin{figure}[t]
\centering
%\includegraphics[width=0.5\textwidth, trim=0.5in 0.5in 0.5in 4.4in, 
%clip=true]{figs/create-latency-cdf-e}
\includegraphics[width=0.8\textwidth]{figs/DELETE-zk-latencies-logscale}
\caption{Latency of ZooKeeper DELETE requests.}
\label{fig:ZooKeeperDELETE}
\end{figure}

Figure~\ref{fig:ZooKeeperDELETE} shows the latency of ZooKeeper DELETE requests 
during the experiment. In the ``Pre'' line, ZooKeeper alone is running in the 
network and no reservations were made using \sys. In the ``Post'' line, we used 
\verb/iperf/ to generate bi-directional TCP flows over each of the six links 
directly connected to a host.
%This traffic totaled 3.3 Gbps, which we found to 
%be the maximum Open vSwitch could sustain in our setup.
As shown in the 
figure, this competing traffic dramatically reduced ZooKeeper's performance -- 
average latency quadrupled from 1.55ms to 6.46ms (we obtained
similar results with a non-OpenFlow switch). Finally, the ``\sys'' line shows 
the return to high performance when ZooKeeper reserved bandwidth using \sys.

We found similar results for other ZooKeeper write operations such as creating 
keys, writing to unique keys, and writing to the same key. Read operations do 
not require a quorum's participation, and thus are less affected by competing background 
traffic.

\subsection{Hadoop}

\begin{figure}[t]
\centering
\subfigure[]{\includegraphics[width=0.46\textwidth]{figs/hadoop_reserv.pdf}}
\subfigure[]{\includegraphics[width=0.46\textwidth]{figs/hadoop_resident_rules.pdf}}
\caption{Effect of Hadoop on \sys and network.}
\label{fig:HadoopResvs}
\end{figure}

In our final case study of \sys's application performance benefits, we augmented
a Hadoop 2.0.3 pre-release with support for our API. Hadoop is an open source
implementation of the MapReduce~\cite{Dean:2008} data-processing framework.
In Hadoop, large files are divided across multiple nodes in the network, and
computations consist of two phases: a map, and a reduce. During the
map phase, a function is evaluated in parallel on independent file pieces.
During the reduce, a second function proceeds in parallel on the collected outputs
of the map phrase; the data transfer from the mappers to the reducers is
known as the shuffle. During the shuffle, every reduce node initiates a
transfer with every map node, making it particularly network-intensive for
some jobs, such as sorts or joins.
Finally, the output of the reduce phase is written back to
the distributed filesystem (HDFS).

By using \sys, our version of Hadoop is able to reserve guaranteed bandwidth
for its operations. The first set of reservations occurs during the shuffle --
each reducer reserves bandwidth for transferring data from the mappers.
The second set of reservations reserves bandwidth when writing the
final output back to HDFS. These few reservations protect the majority of
network transfers that occur during the lifetime of a Hadoop job. Our
version of Hadoop also makes reservations when a map task needs to
read its input across the network; however, such transfers are typically
less common thanks to ``delay scheduling''~\cite{Zaharia:2010}.
Therefore, in a typical job, the total number of reservations is on the order of
$M \times R + R \times 2$ where $M$ and $R$ are, respectively, the
number of nodes with map and reduce tasks. The number of reservations
is not precisely described by this formula as we do not make reservations
for node-local transfers, and reducers may contact a mapper node more
than once during the shuffle phase.
As reducers can either be copying
output from mappers in the shuffle, or writing their output to HDFS,
the maximum number of reservations per reducer at any time is
set by the value of \emph{mapreduce.reduce.shuffle.parallelcopies}
in the configuration, which has a default value of five.

\vskip 0.5em

To measure the effect of using \sys to make reservations in Hadoop,
we developed a benchmark which executed three 40 GB sort jobs
in parallel on a network of 22 machines (20 slaves, plus two masters)
connected by a Pronto 3290 switched controlled by \sys.
Hadoop currently has the ability to prioritize or weight jobs using the
scheduler, but this control does not extend to the network.
In our benchmark,
the first two jobs were provided with 25\% of the cluster's memory
resources, and the third, acting as the ``high priority'' job, was provided
with 50\%. The benchmark was run in two configurations: in the first,
Hadoop made no requests using \sys; in the second, our modified
Hadoop requested guaranteed bandwidth for each large flow.
These reservations were proportional to the job's memory resources,
and lasted for eight seconds, based on Hadoop's 256 MB block size.
In our star topology with uniform 1 Gbps links, this translated to
500 Mbps reservations for each link. 

Averaged across three runs, the high priority job's completion
time decreased by 19\% when its bandwidth was guaranteed.
Because it completed more quickly, the lower priority jobs' runtime also
decreased, by an average of  9\%, since Hadoop's work-conserving scheduler re-allocates
freed memory resources to remaining jobs.

While Hadoop was running, we also measured its effect on \sys and
the switch's flow table. Figure~\ref{fig:HadoopResvs}(a) is a CDF of
the time between Hadoop's reservations. As currently implemented,
\sys modifies the switch flow table after each request. This CDF shows
that batching requests over a 10 ms window would decrease the
number of flow table updates by 20\%; a 100 ms window would decrease
the updates by 35\%. Figure~\ref{fig:HadoopResvs}(b) plots the amount
of flow table space used by Hadoop during a single job. On average,
Hadoop accounted for an additional 2.5 flow table entries; the maximum
number of simultaneous Hadoop rules was 28. 

\section{Implementation Practicality}
\label{sec:Microbenchmarks}

\begin{figure*}[t]
\centering
\subfigure[]{\includegraphics[width=0.32\textwidth]{figs/flow_mod-cdf.pdf}}
\subfigure[]{\includegraphics[width=0.32\textwidth]{figs/createQueue2-cdf.pdf}}
\subfigure[]{\includegraphics[width=0.32\textwidth]{figs/createQueue1-cdf.pdf}}
\caption{Latency of switch operations in milliseconds.}
\label{fig:SwitchOpLatency}
\end{figure*}

In addition to examining \sys's use in real applications, we also evaluated
the practicality of its implementation in current OpenFlow networks.
We found that our Pronto 3290 switches, running the Indigo 2012.09.07
firmware, were capable of supporting 1,919 OpenFlow rules, which took
an average of 7.12 ms to install per rule. To measure this, we developed a
benchmarking controller which installed wildcard match rules, issuing
a barrier request after each \verb/flow_mod/ message was sent. We based this
controller on Floodlight, and it is available for download from our Github page.

The latency distribution to fully install each \verb/flow_mod/ is show in
Figure~\ref{fig:SwitchOpLatency}(a). It has two clusters -- for the 92.4\% of
\verb/flow_mod/'s with latency less than 10.0 ms, the average latency
was 2.80 ms; the remaining 7.6\% had an average latency of 59.5 ms.
For \sys's principals, these much higher tail latencies imply that requests
cannot always be implemented within a few milliseconds, and for truly
guaranteed traffic handling, requests have to be made at least 100
milliseconds in advance.

We found that our Pronto switches could support seven hardware
queues with guaranteed minimum bandwidth on each port, and each
queue required an average of 1.73 ms to create or delete, as shown in
Figure~\ref{fig:SwitchOpLatency}(b). However, this average doubles to 3.56 ms
if queues are created consecutively on the same port (\ie, P1Q1, P1Q2,
P1Q3, ..., P2Q1, etc.), as shown in Figure~\ref{fig:SwitchOpLatency}(c).
This shows that an optimized \sys controller must consider the order in
which switch operations are made to provide the best experience.% for
%its principals.

Together, these results suggest that an
individual switch can support a minimum of about 200 reservations
per second. Higher throughput is possible by batching additional requests
between OpenFlow barriers.
While these switch features are sufficient to support the four applications
above, we found that Hadoop's performance benefited from per-flow
reservations only when flows transferred more than one megabyte. For
smaller flows, the overhead of establishing the reservations outweighed
the benefit.
In an example word count job, only 24\% of flows were greater
than 1 MB; however this percentage rises to 73\% for an example sort.

\section{Related Work}
\label{sec:related-work}

%\remark{
%{\em Expected Outcome:} Reader understands participatory network's relation to 
%previous works, and believes that it provides several novel contributions. 
%Reviewers have been satisfied that their pet papers have been cited.
%}

%\remark{ {\color{red}
%HotSDN reviewer 2 requested a good comparison between these works and
%FML. ``It would be nice to illustrate why these are important (vs. FML's facilities)
%with well-motivated examples that cannot be easily expressed in FML. (see more
%from review)
%} } 


\tightparagraph{Programming the Network}
%
\sys allows applications and users to influence
network operations, a goal shared by previous research such as
active networking~\cite{ActiveNetworking}. In active networks, principals
develop distributed programs that run in the network nodes. By contrast,
\sys sidesteps active networks' deployment challenges via its implementation
as an SDN controller, their security concerns by providing a much more restricted
interface to the network, and their complexity by providing a logically centralized
view of the network.

\tightparagraph{Using Application-Layer Information}
%
Many previous works describe specific cases in which information
from end-users or applications benefits network configuration,
flexibility, or performance; \sys can be a unifying framework for
these.  For example, Hedera~\cite{alfares10hedera} showed that
dynamically identifying and placing large flows in a datacenter can
improve throughput up to 113\%. \sys avoids Hedera's
inference of flow size by enabling applications and devices to
directly inform the network about flow sizes. 
Wang, \etal~\cite{Wang:2012} propose \emph{application-aware networking},
and argue
that distributed applications such as Hadoop can benefit from
communicating their preferences to the network control-plane, as we
show in \S\ref{sec:ApplicationUsage}.
%
ident++~\cite{naous09ident++}
proposes an architecture in which an OpenFlow controller reactively
queries the endpoints of a new flow to determine whether it should
be admitted. 
TVA is a network architecture in which end-hosts authorize the receipt
of packet flows via capabilities in order to prevent DoS-attacks~\cite{Yang:2005}.
%\treelang also supports this goal, but by allowing end-hosts to safely
%manage network policies for themselves, as we show in \xref{sec:ApplicationUsage}.
By contrast, \sys allows administrators to delegate the
privilege to install restricted network-wide firewall rules, and
users can do so either proactively or reactively (\cf
\S\ref{sec:sshguard}).  

%% RF: there could be a discussion (maybe here, maybe in discussion)
%% saying that there are works that do in-band signaling (Mahout, D3,
%% PDQ), and the limits and benefits of in-band and out-of-band signalling
%% are still up for investigation. Nah, this is for another paper, if we can get
%% a good handle on the question.

%UPnP~\cite{upnp}
%allows applications to control a network gateway, such as to add a
%port-forwarding entry to a NAT table, but is restricted to this setting.
%
Darwin~\cite{Darwin} introduced a method for
applications to request use of a network's resources, including
computation and storage capabilities in network processors. Like
\sys, Darwin accounts for resource use hierarchically. However, it
does not support over-subscription, lacks support for
access control and path management, and requires routers with
support for active networks.
%
Yap, \etal have also advocated for an explicit communication channel
between applications and software-defined networks,
in what they called \emph{software-friendly networks} \cite{Yap:2009}.
This earlier work, however, only supports requests made by a single,
trusted application. By contrast, \sys's approach to delegation,
accounting, and conflict-resolution allow multiple applications to
safely communicate with an SDN controller.



%% [RF: D3 is not really a great example, as we can't operate at the same timescales
%% and this would require a longer discussion.
%D$^3$~\cite{wilson11d3} is a transport protocol for datacenter
%networks that replaces TCP's congestion control with explicit rate
%control to meet transmission deadlines.  End-hosts in D$^3$ request
%sending rates based on a flow's size and deadline, and receive
%explicit rate signals from the routers along the flow's path. 
%%The success of D$^3$ illustrates the benefit of using
%%application-level information to allocate bandwidth throughout the
%%network. 
%We plan to test similar mechanisms in \sys using hints about
%flowgroup properties.



\tightparagraph{Network QoS and Reservations}
Providing a predictable network experience is not a new goal, and there is a
vast body of protocols and literature on this topic.  \sys relies
heavily on existing mechanisms, such as reservations and prioritized
queue management~\cite{Kim:2010,Stoica:1997}, while adding
user-level management and resource arbitration.
\sys also goes beyond QoS, integrating  hints
and guarantees about access control and path selection.
To date, we have focused on mechanisms exposed by OpenFlow
switches; we expect other mechanisms for network QoS %, 
%such as those proposed
%for rate-limiting TCP streams in a distributed fashion~\cite{Raghavan:2007}
could be integrated as well.
%\sys's primary contribution is a unified framework for exposing these 
%mechanisms while delegating authority
%and accounting for resource use within a single network. 

Like \sys, protocols such as RSVP~\cite{rfc2205} and NSIS~\cite{rfc5974} 
provide applications with a way to
reserve network resources on the network. \sys, however, is designed
for single administrative domains, which permits centralized control for policy
decisions and accounting, and sidesteps many of their deployment difficulties. 
\sys provides applications with control over the configuration of network paths, which RSVP and NSIS do not, and goes beyond reservations
with its hints, queries, and access control requests, which can be made instantly or for a future
time. Finally, RSVP limits aggregation
support to multicast sessions, unlike \sys's support for flow groups. % (described in Darwin paper)

%% TODO: RF: this should really relate to signaling mechanisms: in band versus out of band.
%% mahout, d3, pdq, XCP, RCP all use in-band signalling, while we use out-of-band signaling
%% they can react faster, and are more scalable, as may are stateless. We can leverage
%% authentication, use multiple applications, handle conflicts, and, while we have to change
%% applications some times, we don't have to change protocols. (Not very strong argument)
%Modern datacenters provide network quality of service for applications by
%assigning packets to priority queues based on header fields, such as DSCP set
%by applications or end-hosts (\eg ~\cite{curtis11mahout}). \sys provides QoS to untrusted principals by configuring queues reactively, an approach which allows the \sys controller to account for
%resource use and enforce allocation policies.
%%\sys provides a network's operators and end-users with an abstraction layer for
%%delegation and accounting over any QoS implementations which exist in the network.
%By contrast, previous approaches to network QoS, which did not provide central
%accounting, are vulnerable to unscrupulous, or mistaken, end-users who mark all
%packets with the highest priority.


Kim, et al.~\cite{Kim:2010} describe an OpenFlow controller which
automatically configures QoS along flow paths using application-described
requirements and a database of network state. \sys's runtime performs
a similar function for the \priv{Reserve} action, and also supports
additional actions.

Recent works in datacenter networks, such as Oktopus~\cite{Ballani:2011} and
CloudNaaS~\cite{Benson2011cloudnaas}, offer a predictable experience to tenants
willing to fully describe their needs as a virtual network, only admitting those tenants
and networks whose needs can be met through careful placement. This approach
is complementary to \sys's, which allows principals to request resources
from an existing network without requiring complete specification.

%This framework provides
%a single API which end-users and applications to can use to improve
%their network experience through any mechanisms which exist within the
%network. 

%priority classes or TOS bits: yes, they are set by the application, but we are doing
%something like a "request to send?" "clear to send" interaction for the priority bits
%... this lets us do accounting and delegation, e.g. for blocks of time when you can
%set the bits to different levels. one of the problems with previous QOS proposals is
%that people always turn the setting up to the max when they're on their own ...
%we are providing an abstraction layer over the network mechanism which
%implements the QOS and provides it to the operator

\tightparagraph{Software-Defined Networking}
%
\sys is part of a line of research into centralized network management including
Onix~\cite{Koponen:2010}, Tesseract~\cite{Tesseract}, and CoolAid~\cite{CoolAid}.
CoolAid provides high-level requests and intentions about the network's configuration
to its operators; \sys extends this functionality to regular users and applications with
the necessary delegation and accounting, and implements them in an SDN.
\sys builds upon the control-plane abstractions proposed by Onix and Tesseract 
for, respectively, OpenFlow and 4D~\cite{Greenberg:2005} networks.

%\tightparagraph{Software-Defined Networking}
%As discussed previously, the current implementation of \sys was developed as
%an application atop the software stack provided by SDNs.
Recent developments in making SDN practical
(\eg,\cite{Gude:2008,McKeown:2008, Voellmy:2011}) greatly improve
the deployability of \sys.  
Resonance~\cite{Nayak:2009} delegates access control to an automated
monitoring system, using OpenFlow to enforce policy decisions.
Resonance could be adapted to use \sys as the mechanism for taking
action on the network, or could be composed with \sys using a
library such as Frenetic~\cite{Foster:2010}.

Expressing policies in a hierarchy is a natural and common way to represent
delegation of authority and support distributed authorship. Cinder~\cite{Roy:2011}, for example, uses a hierarchy of \emph{taps} to provide isolation, delegation, and division of the right to consume
a mobile device's energy.
\sys uses HFTs~\cite{Ferguson:2012b} as a natural way to express, store, and
manipulate these policies
directly, and still enable an efficient, equivalent linear representation of the policy.

FlowVisor~\cite{Sherwood:2010} divides a single network into multiple slices
independently controlled by separate OpenFlow controllers. FlowVisor
supports delegation -- a controller can re-slice its slice of the
network. Each of these controllers sends and receives primitive
OpenFlow messages. In contrast, \sys allows policy authors to
state high-level, declarative policies with flexible conflict resolution.


%The \sys controller
%is built on Nettle~\cite{Voellmy:2011}, a platform for writing OpenFlow
%controllers in a functional reactive style, and allows policy authors to write
%higher-level policies than those Nettle supports natively.



\tightparagraph{Networking and Declarative Languages}
\sys's design is inspired by projects such as the Margrave tool for firewall 
analysis~\cite{n++:lisa-margrave-firewalls} and the Router Configuration
Checker~\cite{Feamster:2005}, which apply declarative languages to
network configuration. Both use a high-level language to detect configuration 
mistakes in network policies by checking against predefined constraints. 
\sys, however, directly integrates such logic into the network
controller.
%NDlog~\cite{Loo:2009} adapted the declarative Datalog language to ease the 
%programming of distributed applications and protocols; \sys receives
%requests from distributed clients, but their evaluation is centralized.

FML~\cite{Hinrichs:2009} is a Datalog-inspired language for writing
policies that also supports distributed authorship.  
The actions in \sys are inspired by
FML, which it
extends by involving end-users, adding queries and hints, and
introducing a time dimension to action requests.
In an FML policy,
conflicts are resolved by a fixed scheme -- deny overrides waypoints,
and waypoints override allow. By contrast, \sys offers more
flexible conflict resolution operators.
%For example, within a single \treelang policy tree,
%one policy node may specify ``allow overrides deny,'' while another
%specifies ``deny overrides allow.''
%
FML also allows policies to be prioritized in a linear sequence (a
\emph{policy cascade}). \sys can also express a prioritized sequence of
policies, in addition to more general hierarchies.  
%For example, \sys
%uses an inverted ``child overrides parent'' conflict-resolution scheme
%(\xref{sec:conflict-resolution-operators}) by default, and the author of an individual
%policy node can adopt a more restrictive ``parent overrides child''
%scheme. FML does not support both ``child overrides parent'' and
%``parent overrides child'' schemes simultaneously.


{\color{red}
%Sharing data center network \cite{S hieh:2011, Popa:2011}.
%Predictable datacenter networks \cite{Ballani:2011}.

%Do we want to say anything about how we compare to the IETF 82 BOF session on 
%``Software Driven Networks''?
%\url{https://www.ietf.org/proceedings/82/minutes/sdn.html}
%\url{http://lucidvision.com/pipermail/sdnp/}
}

%\treelang gives policy writers a programming model in which a central
%controller sees all packets. This model is inspired by Frenetic~\cite{Foster:2010}, which uses 
%NetCore~\cite{Monsanto:2012} as its
%language for expressing forwarding policies. 
%In contrast to NetCore,
%\treelang supports \emph{hierarchical policies}, which naturally support
%distributed authorship. A key element of \treelang is allowing
%overlapping and conflicting policies to co-exist in a policy tree,
%as it resolves conflicts with arbitrary, user-defined
%operators.
%
%NetCore allows policy writers to match packets with arbitrary
%predicates; when predicates are not realizable on switches, packets
%are sent to the NetCore controller, which uses \emph{reactive
%  specialization} to install exact-match rules. In contrast,
%\treelang's matching rules are simpler and realizable on OpenFlow
%switches, though we do not anticipate any problem supporting reactive
%specialization.


% see: http://books.google.com/books?id=EMiBsbiYA-kC&pg=PA31&lpg=PA31
The eXtensible Access Control Markup Language (XACML) provides
four combiner functions to resolve conflicts between subpolicies~\cite{gm:xacml-1.1}.
These functions are designed for access control decisions and assume an ordering
over the subpolicies. % eg, ``first-applicable''
By contrast, HFTs support user-supplied operators designed for several actions
and consider all children equal.
